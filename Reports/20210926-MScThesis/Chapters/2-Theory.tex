
\chapter{Theory: Methods and Distances}

A mutlitude of algorithms exist that embed pr project high dimensional data in lower dimensional space. THis thesis down't aim at giving an overview or comparison. I will in the next section rather describe why the chosen t-SNE algorithm works as well as any other to show the main result of this thesis: the Structure in Covaraince of hierarchical data. All visualizations in this report could as well have been plotted using PCA, MDS, UMAP or any other algorithm, in particluar even using  t-SNE but with different hyperparamerters. The contribution of this work shows however, that within one algorithm it makes much of a difference, if we use as distance measure in the high dimensional space only the distance of means or more, potentially full, information. Nevertheless, in order to understand the concept of dimension reduction, I will briely summaraize the relevant details of the most prominent algorithms and show their embedding on the European Values STudy dataset, whcih will be further analyzes in section \ref{EVS}. ALso I will describe the shortcomings and advantages of the Wasserstein distance as a possible metric in the high dimensional space, as well as its computation, in section \ref{WT:histogram}.

\section{Dimension Reduction}

Let us consider a dataset $\X$ that contains $N$ datapoints $ \X_n \in \Re^D$. We now want to find a two dimneional dataset $\Y \in \Re^{N\times 2}$ that has similar structure. It is possible to project or embed $\X$ or use other methods for example generative models. In this section I will explain a projection(PCA) and two embeddings (t-SNE, UMAP). 

\subsection{PCA}
\label{PCA}

Principal Component Analzysis finds the axes of largest variation in the dataset and projects every datapoint onto these vectors. Specifically, we look for two vectors $v_1, v_2 \in \Re^D$ such that the following expression is maximized

\begin{equation}
\min_{v_1,v_2 \in \Re^D} \var{\X v_1} + \var{\X v_2}
\end{equation}


\subsection{t-SNE}
\label{tSNE}
A non-linear alternative in Dimension  Reduction is called t Stochastic Neighbor Embedding. We define a distance measure such as Euclidean distance in the high dimensional space and t

RThe points are then moved along the gradient until it results in a local maximum where no point can be slighlty moved without resulting in a worse embeding than before

When interpreting t-SNE embedding is is important to keep in mind that the choice of distance measure puts emphasis on close points. Points the are embedded far from each other don't need to be far away in the high dimensional dtaset. 

Initilazation



\section{Wasserstein Distance}

We have learned that for a meaningful embedding it is necessary to have a distance measure in the high diomensional space. When dealing with hierarchical data this becomes non trivial as there is no default distacne measure for probability distribution. A standard way is to collapse the dsitrbition into the mean and then use the Euclidian distance of the means. But one can easily imagine that this technique can loose arbitryily much of the information. A standard metric in computer Science is called the Wasserstein metric and has been widely used to compare distibutions. Its downside is the complex compuation durance. However, for small datasets we have been able to counter this problem and computed Wasserstein distances. I will thus give a brief overview of the theory and then explain the computation.


\subsection{Exact Formulation}

The Wasserstein metrix is formally edfined by

\subsection{Special Case: Gaussians}

For multivariate normals there exists a closed form solution of the 2-Wasserstein metric. it reads

formalu

and the derivation can be found in Frechet distance blabla


\subsection{Convex Interpolation Method}

We first nore, that the first part of the sum is just the Euclidiean distance of the mean of the distribution. The Wasserstein distance can therfore be seen as an extension of the Euclidiean distance. As Landau etc showed, the second summand in euqation 1 is a proper meric on covarainces. We can therefore combine these two distances in any way other than 

This leads to the convex generalization of the Wasserstein distance which ziels both other distances for $\lambda = 0$ or $\lambda = 1$ respectively.
 
\section{Linear Programming}

We have earlier stated that the Wasserstein distance is hard to compute for continuous distributions. However for discrete distributions it boils down to the linear program described in Equaton 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Wasserstein/Experiments1D.pdf}
	\caption{Linear Program}
	\label{WT:linprog}
\end{figure}


\subsection{Scalability in Participants}

If the metric space on which the distributions are defined is small, a linear programm can solve the wasserstein distance with unique optimal solution. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Wasserstein/ExperimentsHistogram.pdf}
	\caption{Wasserstein Histogram}
	\label{WT:histogram}
\end{figure}


\subsection{Scalability in Features}

When dealing with larger spaces the linear program grows exponentially in the number of dimensions. A discrete space with 10 values yields a linear programm with 100 variables, n features yiled a linear programm with $10^{2n}$ variables. However, since every variable must be non negative and every row/column must sum to the respetive marginal distribution, each zero in a marginal distribution forces the whole colum/row to contain zeros as well. Thus we can exclude these variable from the ;linear programm and eventually arrive at an upper bound of $N\cdot M$ variables for marginal distributions with $N$ and $M$ samples respectively. This approach is indifferent to the number of features, since we can compute the pairwise Euclidean distance of the samples efficiently using vectorization. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Wasserstein/ExperimentsUniform.pdf}
	\caption{Wasserstein Uniform}
	\label{WT:uniform}
\end{figure}
